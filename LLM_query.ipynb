{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e00518a-478a-46c4-b7b4-270e35669a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeef8833-0b11-4f7f-99e2-e3559c2d6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9022ac50-6830-444b-b1f0-28912b710abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting install\n",
      "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/skris142/.local/lib/python3.11/site-packages (from sentence_transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/skris142/.local/lib/python3.11/site-packages (from sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sentence_transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/skris142/.local/lib/python3.11/site-packages (from sentence_transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sentence_transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.29.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/skris142/.local/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/skris142/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/skris142/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.10)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/skris142/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/skris142/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: install, sentence_transformers\n",
      "Successfully installed install-1.3.5 sentence_transformers-2.7.0\n"
     ]
    }
   ],
   "source": [
    "#Embedding\n",
    "!pip install install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a5e7f5-8e26-4042-b892-0bbe6298651a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama_index in /home/skris142/.local/lib/python3.11/site-packages (0.10.36)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.2.4)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.10.36)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.9)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.18)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: openai>=1.14.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.28.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.9.2)\n",
      "Requirement already satisfied: httpx in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.26.0)\n",
      "Requirement already satisfied: pandas in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.1.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.14.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/skris142/.local/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.7.1)\n",
      "Requirement already satisfied: anyio in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (4.0.0)\n",
      "Requirement already satisfied: certifi in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/skris142/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.0.5)\n",
      "Requirement already satisfied: idna in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.4)\n",
      "Requirement already satisfied: sniffio in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/skris142/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.14.0)\n",
      "Requirement already satisfied: click in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/skris142/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/skris142/.local/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.26.16)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/skris142/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/skris142/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/skris142/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/skris142/.local/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama_index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/skris142/.local/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/skris142/.local/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index-llms-huggingface\n",
      "  Downloading llama_index_llms_huggingface-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-llms-huggingface) (0.23.0)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-llms-huggingface) (0.10.36)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-llms-huggingface) (2.2.2)\n",
      "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-llms-huggingface) (4.40.2)\n",
      "Requirement already satisfied: filelock in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/skris142/.local/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.8.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.26.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.28.0)\n",
      "Requirement already satisfied: pandas in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.1.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (10.0.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/skris142/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.14.1)\n",
      "Requirement already satisfied: pydantic<3,>2 in /home/skris142/.local/lib/python3.11/site-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.7.1)\n",
      "Requirement already satisfied: sympy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
      "Requirement already satisfied: jinja2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/skris142/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/skris142/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/skris142/.local/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.10)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/skris142/.local/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/skris142/.local/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/skris142/.local/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.30.1)\n",
      "Requirement already satisfied: psutil in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/skris142/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
      "Requirement already satisfied: anyio in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.0.0)\n",
      "Requirement already satisfied: certifi in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/skris142/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
      "Requirement already satisfied: idna in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.4)\n",
      "Requirement already satisfied: sniffio in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/skris142/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
      "Requirement already satisfied: click in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/skris142/.local/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/skris142/.local/lib/python3.11/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/skris142/.local/lib/python3.11/site-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (1.26.16)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/skris142/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/skris142/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/skris142/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
      "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
      "Successfully installed llama-index-llms-huggingface-0.2.0 text-generation-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n",
    "!pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aabe3253-2bde-49d6-8103-ccb8f87228f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skris142/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "#from llama_index.llms import HuggingFaceLLM\n",
    "#from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "#from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "#from llama_index.core import SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
    "#from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3751de20-208d-4998-9d82-9004253e4074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='c527b3bb-bfee-41a4-a52b-94255de9ba00', embedding=None, metadata={'page_label': '1', 'file_name': 'Project Report #1.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #1.pdf', 'file_type': 'application/pdf', 'file_size': 1875518, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Orthographic Feature Transform for Monocular 3D\\nObject Detection\\nReport by :\\nShreya Krishna\\n1 Introduction 1\\nIn the domain of computer vision and robotics, accurately detecting and localizing objects within a 2\\nscene is crucial, especially for advancing autonomous systems. This underscores the importance of 3\\n3D object detection, which involves precisely determining the spatial positions and dimensions of 4\\nobjects for tasks like prediction, avoidance, and path planning. 5\\nTraditionally, 3D object detection methods have heavily relied on LiDAR point clouds for their 6\\ndetailed spatial information, enabling successful object localization and recognition. However, 7\\nLiDAR’s high cost, sparse point clouds at longer ranges, and the need for sensor redundancy present 8\\nchallenges.[1], [2], [3] 9\\nIn contrast, approaches based solely on monocular RGB images have lagged behind due to limitations 10\\nlike varying object scales with distance, viewpoint-dependent appearances, and the inability to directly 11\\ninfer distances in 3D space.[4], [5] 12\\nTo overcome these challenges, a novel strategy is being pursued, focusing on reasoning in a three- 13\\ndimensional space rather than being confined to pixel-based image representations. Inspired by 14\\northographic bird’s-eye-view maps commonly used in LiDAR-based techniques, this approach aims 15\\nto convert perspective image-based features into an orthographic representation. [6] 16\\n2 Solution 17\\nAn innovative 3D object detection algorithm utilizes a lone monocular RGB image to generate precise 18\\n3D bounding boxes, showcasing superior performance compared to other monocular methods on 19\\nthe rigorous KITTI benchmark. This groundbreaking system introduces the orthographic feature 20\\ntransform (OFT) as a key component. [6], [7] 21\\nThe Orthographic Feature Transform (OFT) is introduced to enable reasoning about the 3D world 22\\nwithout the effects of perspective projection. This transformation involves mapping feature maps 23\\nextracted from image space to orthographic feature maps in world space. The voxel feature map 24\\nobtained from this transformation represents the scene without perspective effects but is memory- 25\\nintensive for deep neural networks.[8] 26\\nTo address memory constraints, the 3D voxel feature map is collapsed into a two-dimensional 27\\northographic feature map. This collapsing is achieved by summing voxel features along the vertical 28\\naxis after multiplication with learned weight matrices. Retaining information about the vertical 29\\nconfiguration of the scene is crucial for tasks such as estimating the height and vertical position of 30\\nobject bounding boxes. 31\\nTo facilitate pooling over a large number of regions, a fast average pooling operation based on integral 32\\nimages is employed. This operation computes output features corresponding to defined regions 33\\nefficiently, regardless of region size or shape, making it suitable for varying voxel distances from the 34', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7975a9ac-2ea3-41bf-b4f8-82d1764c5135', embedding=None, metadata={'page_label': '2', 'file_name': 'Project Report #1.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #1.pdf', 'file_type': 'application/pdf', 'file_size': 1875518, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='camera. Additionally, it is fully differentiable, allowing integration into an end-to-end deep learning 35\\nframework. 36\\n3 Result 37\\nThe experiment utilizes the KITTI 3D object benchmark dataset, containing 15,000 images. These 38\\nimages are partitioned into 3,712 training images, 3,769 validation images, and 7,518 test images. 39\\nTraining is conducted over 600 epochs, employing a learning rate of 10e-9, with ResNet-18 serving 40\\nas the frontend network architecture. 41\\nDuring our object detection experiments using the KITTI dataset, we initially trained the model for 42\\nonly 100 epochs. However, we encountered numerous inaccurate bounding boxes with incorrect 43\\norientation, prompting us to experiment further. Training on half of the dataset yielded even worse 44\\nresults, with many irrelevant bounding boxes detected. To address these issues, we extended the train- 45\\ning duration to 600 epochs, resulting in a slight decrease in irrelevant bounding boxes. However, the 46\\nhighest Intersection over Union (IoU) achieved was only 26%, indicating subpar overall performance, 47\\nwith significant problems in orientation, dimensions, and logic of detected objects. 48\\nTo enhance feature extraction, we explored utilizing a superior network such as ResNet34. Although 49\\nloss convergence remained similar, the average precision numbers improved notably on testing data. 50\\nDespite some improvement, bounding boxes still exhibited slight orientation issues. We mitigated this 51\\nby implementing non-maximum suppression and refining results with additional IoU thresholding. 52\\nAttempts to improve performance with ResNet50 and increased layer depth yielded worse results 53\\nthan ResNet18. Decreasing the learning rate and training on dimly lit images improved detection 54\\naccuracy under low light conditions, while training with adjusted angle weights addressed orientation 55\\ninaccuracies, albeit slowing down angle loss convergence. 56\\nOut of all approaches, the ResNet34 model emerged as the most effective, demonstrating exceptional 57\\ncar detection capabilities, particularly at greater depths. Visual comparisons between baseline and 58\\nimproved testing results highlighted the significant performance difference of the ResNet34 model. 59\\nIts relatively shallow architecture allows for better reasoning during training, making it less prone to 60\\noverfitting and more efficient in extracting meaningful features. 61\\nFigure 1: Testing Results of different approachs\\nIn conclusion, our findings indicate that the ResNet34 model is a top choice for car detection tasks, 62\\nespecially in scenarios involving large datasets or complex environments where detecting cars at 63\\nvarying depths is crucial. We anticipate its continued value in the realms of computer vision and 64\\nimage processing. 65\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3961b7a2-7f69-4c49-a18a-69526e509df1', embedding=None, metadata={'page_label': '3', 'file_name': 'Project Report #1.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #1.pdf', 'file_type': 'application/pdf', 'file_size': 1875518, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 2: Loss plot comparison\\n4 Contribution 66\\nEngaging in this project led me to explore diverse and innovative approaches. Upon initiating 67\\nthe repository setup on my personal laptop, I discerned a need for superior computational power, 68\\nprompting me to leverage the university’s supercomputer resources. Harnessing a V100 GPU, I 69\\noptimized the intricate computations, substantially enhancing efficiency. The initial training spanned 70\\nfive days, which spurred me to revise the code for parallel computations across 3-4 A100 GPUs, 71\\nresulting in a remarkable 60% reduction in training time, concluding in a mere two days. 72\\nThe project commenced with ResNet18 for feature extraction, dynamically training weights at a 73\\nlearning rate of 10e-7. Evaluating the model’s performance with pretrained weights unveiled marginal 74\\nvariance. To delve deeper, I ventured into more sophisticated architectures like ResNet50 and 75\\nResNet101. Methodically comparing outcomes, I fine-tuned parameters such as NMS thresholds, 76\\nbatch size, and learning rate. A configuration featuring a NMS threshold of 0.025, batch size of 4, and 77\\na learning rate of 10e-9 proved optimal for the available dataset, particularly in tandem with ResNet34 78\\nas the feature extractor. I engineered testing code for precise metrics, encompassing Intersection over 79\\nUnion and accuracy. Augmenting model cognition, an additional threshold was incorporated during 80\\ntesting, resulting in enhanced reasoning capabilities. 81\\nMoreover, I created a comprehensive comparative graph, succinctly illustrating the performance of all 82\\nemployed neural networks. This graph showcased a notable enhancement over the original reference 83\\nproject, providing a clear visual representation of the progress achieved. 84\\nList of team members : Shreya Krishna, Zehnaseeb Ali, Pranav Gajanan Chougule and Neel Macwan. 85\\n5 Key Takeaways 86\\nParticipating in this project has been an enlightening experience. It underscored the critical importance 87\\nof computational capabilities, prompting me to explore the realm of ML accelerators suited for 88\\nresource-intensive tasks. Beyond that, I gained profound understanding of the architecture, complexity, 89\\nand distinctive attributes of various convolutional neural networks. I also acquired a comprehensive 90\\nunderstanding of the overarching workflow in machine learning. 91\\nThe project introduced me to the challenge of over-fitting, a hurdle resolved through prudent applica- 92\\ntion of cross-validation techniques and a more diverse dataset. I also mastered diverse methods for 93\\ncalculating accuracy. Notably, I grasped the potential impact – a successful and refined implementa- 94\\ntion of this project could potentially reduce autonomous vehicle manufacturing costs by an estimated 95\\n20%. 96\\nOverall, this project has not only bolstered my technical and academic proficiency but has also 97\\nfurnished a panoramic view of the potential benefits and future prospects in this domain. 98\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c8615b3-fbe1-48b3-82da-080376392d56', embedding=None, metadata={'page_label': '4', 'file_name': 'Project Report #1.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #1.pdf', 'file_type': 'application/pdf', 'file_size': 1875518, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 3: Improved Testing Results on Resnet 34\\n6 References 99\\n[1] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In 100\\nECCV , pages 483–499, 2016 101\\n[2] X. Du, M. H. Ang Jr, S. Karaman, and D. Rus. A general pipeline for 3D detection of vehicles. 102\\narXiv preprint arXiv:1803.00387, 2018. 103\\n[3] K. Minemura, H. Liau, A. Monrroy, and S. Kato. Lmnet: Real-time multiclass object detection 104\\non CPU using 3D LiDARs. arXiv preprint arXiv:1805.04902, 2018. 105\\n[4] B. Li, T. Zhang, and T. Xia. Vehicle detection from 3D lidar using fully convolutional network. 106\\narXiv preprint arXiv:1608.07916, 2016. 107\\n[5] S.Wirges, T. Fischer, J. B. Frias, and C. Stiller. Object detection and classification in occupancy 108\\ngrid maps using deep convolutional networks. arXiv preprint arXiv:1805.08689, 2018. 109\\n[6] S.-L. Yu, T. Westfechtel, R. Hamada, K. Ohno, and S. Tadokoro. Vehicle detection and localization 110\\non birds eye view elevation images using convolutional neural network. In IEEE International 111\\nSymposium on Safety, Security and Rescue Robotics (SSRR), volume 5, 2017. 112\\n[7] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun. Monocular 3D object detection 113\\nfor autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern 114\\nRecognition, pages 2147–2156, 2016. 115\\n[8] .Zechen Liu, Zizhang Wu, Roland Toth; SMOKE: Single-Stage Monocular 3D Object Detection 116\\nvia Keypoint Estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 117\\nRecognition (CVPR) Workshops, 2020. 118\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b11822a-27c1-478c-8ff5-0a47f3db3545', embedding=None, metadata={'page_label': '1', 'file_name': 'Project Report #2.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #2.pdf', 'file_type': 'application/pdf', 'file_size': 695827, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Parrot Mambo Mini-Drone Project\\nReport by :\\nShreya Krishna\\n1 Introduction 1\\nThis project, aim to explore the capabilities of the Parrot Mambo mini drone by implementing various 2\\ncontrol and image processing techniques using MATLAB [1]. The project starts with a fundamental 3\\nunderstanding of the drone’s hardware, including motor control and wireless communication setup. 4\\nSubsequently, I delve into path planning algorithms to guide the drone along predefined trajectories. 5\\nFurthermore, we implement color detection algorithms to enable the drone to recognize and follow 6\\nspecific colors, facilitating tasks like tracking lines or objects of interest. Additionally, we leverage 7\\nadvanced techniques such as object detection to further enhance the drone’s ability to detect shapes 8\\nand colors, enabling more sophisticated interaction with its environment. 9\\nFigure 1: Parrot mambo mini drone\\n2 Solution 10\\nTo address the objectives outlined above, we commenced the project by thoroughly examining the 11\\nhardware of the Parrot Mambo mini drone, focusing on motor control and wireless communication 12\\nsetup. MATLAB’s support packages provided convenient tools for establishing TCP/IP and UDP 13\\nconnections, enabling wireless control of the drone [2],[3],[4]. Next, we utilized path planning 14\\nalgorithms to define and execute specific flight paths, such as rectangular trajectories and orbits, 15\\nusing the waypoint follower block in the path planning module. To enhance the drone’s perception 16\\ncapabilities, we implemented color detection algorithms, starting with color space conversion to 17\\nHSV and thresholding to isolate regions of interest, particularly red lines [5],[6]. Further processing 18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ef0653c-627d-4cc0-a1ca-c4f04be00355', embedding=None, metadata={'page_label': '2', 'file_name': 'Project Report #2.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #2.pdf', 'file_type': 'application/pdf', 'file_size': 695827, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='involved filtering, segmentation, and blob detection to accurately identify and track the red line. 19\\nAdditionally, we employed object detection to detect shapes and colors, enabling the drone to classify 20\\nobjects and respond dynamically based on the classification results [7],[8]. By combining these 21\\ntechniques, we successfully empowered the Parrot Mambo mini drone with enhanced control and 22\\nperception capabilities, paving the way for a wide range of autonomous navigation and interaction 23\\ntasks. 24\\n3 Result 25\\nThe implementation of various control and image processing techniques yielded promising results, 26\\nsignificantly enhancing the capabilities of the Parrot Mambo mini drone. In the hardware explo- 27\\nration phase, successful motor control tests confirmed the drone’s responsiveness to commands, 28\\nensuring smooth operation. The establishment of TCP/IP and UDP connections facilitated wireless 29\\ncommunication, enabling remote control of the drone. Path planning algorithms effectively guided 30\\nthe drone along predefined trajectories, demonstrating precise and accurate navigation capabilities. 31\\nThe color detection algorithm successfully converted RGB color values into the HSV color space, 32\\nenabling robust detection and tracking of specific colors, particularly red lines. Through thresholding, 33\\nfiltering, segmentation, and blob detection, the drone reliably identified and followed the designated 34\\nred line, showcasing its ability to interact with its environment based on color cues. Leveraging 35\\nobject detection further expanded the drone’s perceptual abilities, enabling it to detect and classify 36\\nvarious shapes and colors with high accuracy. The integration of feedback systems allowed the 37\\ndrone to respond dynamically to classification results, demonstrating adaptability and versatility in 38\\ndifferent scenarios. Overall, the results highlight the effectiveness of the implemented techniques in 39\\nenhancing the Parrot Mambo mini drone’s control and perception capabilities, laying the foundation 40\\nfor advanced autonomous navigation and interaction tasks. 41\\nFigure 2: Red line the drone followed Successfully\\n4 Possible Applications 42\\nThe enhanced capabilities of the Parrot Mambo mini drone, facilitated by the implemented control 43\\nand image processing techniques, open up a wide range of potential applications across various 44\\ndomains. In the field of robotics education and research, the drone serves as an ideal platform for 45\\nhands-on learning and experimentation, allowing students and researchers to explore concepts such 46\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2dad896f-57b0-47eb-af9d-5fd4a2b8a930', embedding=None, metadata={'page_label': '3', 'file_name': 'Project Report #2.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #2.pdf', 'file_type': 'application/pdf', 'file_size': 695827, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='as motor control, wireless communication, path planning, and computer vision in a practical and 47\\nengaging manner. Additionally, in the realm of industrial automation, the drone’s ability to navigate 48\\npredefined paths and detect specific colors could be utilized for tasks such as inventory management, 49\\nwarehouse monitoring, and automated inspection processes. Moreover, in the field of entertainment 50\\nand events, the drone’s enhanced capabilities enable captivating aerial performances and interactive 51\\nexperiences, enhancing audience engagement and creating memorable moments. Furthermore, in 52\\nthe agricultural sector, the drone’s ability to navigate and identify specific objects or areas could be 53\\nleveraged for tasks such as crop monitoring, pest detection, and precision agriculture. Overall, the 54\\nversatile applications of the Parrot Mambo mini drone empower users to explore innovative solutions 55\\nacross various industries, driving advancements in automation, entertainment, education, and beyond. 56\\n5 Contribution 57\\nThis project significantly contributes to the exploration and enhancement of the capabilities of the 58\\nParrot Mambo mini drone through the implementation of various control and image processing 59\\ntechniques using MATLAB. By thoroughly understanding the drone’s hardware and leveraging 60\\nMATLAB’s support packages, I successfully established TCP/IP and UDP connections for wireless 61\\ncontrol of the drone, laying the foundation for further experimentation. The project advanced with the 62\\nimplementation of path planning algorithms, enabling the drone to navigate predefined trajectories 63\\nwith precision, enhancing its autonomous flight capabilities. Subsequently, I implemented color 64\\ndetection algorithms to enable the drone to recognize and follow specific colors, facilitating tasks like 65\\ntracking lines or objects of interest. Moreover, by incorporating advanced techniques such as object 66\\ndetection, I further augmented the drone’s perception capabilities, allowing it to detect shapes and 67\\ncolors and dynamically respond based on the classification results. Overall, this project contributes to 68\\nthe field of robotics and computer vision by showcasing practical applications of control and image 69\\nprocessing techniques in enhancing the functionality and autonomy of drones. This project was an 70\\nindividual effort so I did everything on my own. 71\\nSteps Followed: 72\\n1. Thorough examination of the Parrot Mambo mini drone’s hardware, focusing on motor control and 73\\nwireless communication setup. 74\\n2. Utilization of MATLAB’s support packages to establish TCP/IP and UDP connections for wireless 75\\ncontrol of the drone. 76\\n3. Implementation of path planning algorithms using MATLAB’s waypoint follower block to define 77\\nand execute specific flight paths, such as rectangular trajectories and orbits. 78\\n4. Implementation of color detection algorithms, starting with color space conversion to HSV and 79\\nthresholding to isolate regions of interest, particularly red lines. 80\\n5. Further processing of the thresholded images involving filtering, segmentation, and blob detection 81\\nto accurately identify and track the red line. 82\\n6. Employment of object detection techniques to detect shapes and colors, enabling the drone to 83\\nclassify objects and respond dynamically based on the classification results. 84\\n7. Integration of the implemented techniques to empower the Parrot Mambo mini drone with enhanced 85\\ncontrol and perception capabilities, enabling autonomous navigation and interaction tasks. 86\\n6 Key Takeaways 87\\nExploration of Drone Capabilities: This project aimed to explore the potential of the Parrot Mambo 88\\nmini drone by implementing various control and image processing techniques using MATLAB. 89\\nFundamental Understanding: The project began with a comprehensive understanding of the drone’s 90\\nhardware, focusing on motor control and wireless communication setup. This foundational knowledge 91\\nlaid the groundwork for subsequent tasks. 92\\nPath Planning Algorithms: Path planning algorithms were utilized to guide the drone along predefined 93\\ntrajectories, such as rectangular paths and orbits. The waypoint follower block in the path planning 94\\nmodule facilitated the execution of specific flight paths. 95\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffdb5111-c66e-47d8-af38-514214f7c0d8', embedding=None, metadata={'page_label': '4', 'file_name': 'Project Report #2.pdf', 'file_path': '/home/skris142/LLM/data/Project Report #2.pdf', 'file_type': 'application/pdf', 'file_size': 695827, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Color Detection and Recognition: The project implemented color detection algorithms to enable 96\\nthe drone to recognize and follow specific colors, particularly red lines. Techniques such as color 97\\nspace conversion to HSV , thresholding, filtering, segmentation, and blob detection were employed to 98\\naccurately identify and track the desired color. 99\\nObject Detection: Advanced techniques, including object detection, were leveraged to enhance the 100\\ndrone’s ability to detect shapes and colors. By classifying objects in its environment, the drone could 101\\nrespond dynamically based on the classification results. 102\\nEmpowering the Drone: Through the integration of various control and image processing techniques, 103\\nthe project successfully empowered the Parrot Mambo mini drone with enhanced control and per- 104\\nception capabilities. This enables the drone to perform a wide range of autonomous navigation and 105\\ninteraction tasks effectively. 106\\n7 References 107\\n[1] Mathworks parrot minidrone 108\\n[2] Spin the Motors of a Parrot Minidrone Without Flying the Drone 109\\n[3] Controlling Motor Speed of a Parrot Minidrone During Runtime Using External Mode 110\\n[4] Communicating with a Parrot Minidrone Using TCP/IP and UDP 111\\n[5] Getting Started with Image Processing Algorithms for Parrot Minidrones 112\\n[6] Drone Simulation and Control 113\\n[7] Programming Drones with Simulink 114\\n[8] Follow Set of Waypoints using Parrot MiniDrone 115\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=SimpleDirectoryReader(\"./data\").load_data()\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5a7c50a-41ba-4156-887c-ed2de0a2ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='<|USER|>{query_str}<|ASSISTANT|>')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt=\"\"\"\n",
    "You are a Q&A assistant. Your goal is to answer questions as \n",
    "accurately as possible based on the instructions and context provided.\n",
    "\"\"\"\n",
    "#we will use a default format which is supported by llama2\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "query_wrapper_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491ebbb-efa5-49cd-943a-9eb4a8bb3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331b158-0f4d-4fd9-b2f8-80b68332253a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
